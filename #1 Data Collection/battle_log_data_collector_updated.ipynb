{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf0d6f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aseel\\.conda\\envs\\da12\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f32aba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d, parent_key='', sep='_'):\n",
    "    \"\"\"\n",
    "    Flatten a nested dictionary.\n",
    "    \n",
    "    Args:\n",
    "        d: Dictionary to flatten\n",
    "        parent_key: String to prepend to dictionary keys\n",
    "        sep: Separator between parent and child keys\n",
    "    \n",
    "    Returns:\n",
    "        Flattened dictionary\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        \n",
    "        if isinstance(v, dict):\n",
    "            # Recursively flatten nested dictionaries\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "\n",
    "def flatter_battle_log(dict_battle:dict)->dict:\n",
    "    new_dict= {}\n",
    "\n",
    "    dict_keys = [\"game_config\"]\n",
    "    list_keys = [\"players\"]\n",
    "    remove_keys = []\n",
    "    for key in dict_battle.keys():\n",
    "        if key in dict_keys:\n",
    "            flattened = flatten_dict(dict_battle[key], parent_key=key)\n",
    "            new_dict.update(flattened)\n",
    "        elif key in list_keys:\n",
    "            for i in range(len(dict_battle[key])):\n",
    "                flattened = flatten_dict(dict_battle[key][i], parent_key=f\"{key}_{i}\")\n",
    "                new_dict.update(flattened)\n",
    "        \n",
    "        elif key in remove_keys:\n",
    "            continue\n",
    "        else:\n",
    "            new_dict[key] = dict_battle[key]\n",
    "\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08cd6ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://stats-royale-api-js-beta-z2msk5bu3q-uk.a.run.app/profile/2G2L9PRUP\n",
      "Completed: 2G2L9PRUP - Status: success\n",
      "Data saved to battle_log_data.csv\n"
     ]
    }
   ],
   "source": [
    "def fetch_api_data(\n",
    "    id: str,\n",
    "    url: str = \"https://stats-royale-api-js-beta-z2msk5bu3q-uk.a.run.app/profile/\",\n",
    "    headers: Dict = None,\n",
    ") -> Dict:\n",
    "    \"\"\"Fetch data from a single API endpoint\"\"\"\n",
    "    try:\n",
    "        request_url = f\"{url}{id}\"\n",
    "        print(f\"Fetching: {request_url}\")\n",
    "        response = requests.get(request_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        # if response.status_code == 200:\n",
    "        #     matches = response.json()['matches']\n",
    "        #     flattened_matches = [flatten_dict(match) for match in matches]\n",
    "        #     return {\"id\": id, \"flat_matches\": flattened_matches, \"status\": \"success\"}\n",
    "        matches = response.json()['matches']\n",
    "        flattened_matches = [flatter_battle_log(match) for match in matches]\n",
    "        df_matches = pd.DataFrame(flattened_matches)\n",
    "\n",
    "        return id,\"success\",df_matches\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"error\",e)\n",
    "        return \"failed\", None\n",
    "\n",
    "\n",
    "def parallel_api_calls(\n",
    "    ids: List[str], max_workers: int = 5, headers: Dict = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Make parallel API calls\n",
    "\n",
    "    Args:\n",
    "        urls: List of API URLs to call\n",
    "        max_workers: Number of parallel workers (default: 5)\n",
    "        headers: Optional headers for API requests\n",
    "\n",
    "    Returns:\n",
    "        List of results from all API calls\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_id = {executor.submit(fetch_api_data, id): id for id in ids}\n",
    "        results = pd.DataFrame()\n",
    "        # Process completed tasks\n",
    "        for future in as_completed(future_to_id):\n",
    "            result = future.result()\n",
    "            results = pd.concat([results,result[2]],ignore_index=True)\n",
    "            print(f\"Completed: {result[0]} - Status: {result[1]}\")\n",
    "            \n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "def save_to_csv(data: pd.DataFrame, filename: str):\n",
    "    \"\"\"Save DataFrame to CSV file\"\"\"\n",
    "    data.to_csv(filename, index=False)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "# Example usage:\n",
    "# Define your Player ids\n",
    "ids = [\"2G2L9PRUP\"]\n",
    "\n",
    "# Optional: Add headers if needed (e.g., API key)\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36\",\n",
    "}\n",
    "\n",
    "# Make parallel API calls\n",
    "results_df = parallel_api_calls(ids, max_workers=10, headers=headers)\n",
    "\n",
    "# Save to CSV\n",
    "save_to_csv(results_df, \"battle_log_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5859e35c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57b0099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data series with each row is a list of json strings, for every entry in the list, we need to be a new row in new df with key being the column name\\\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
